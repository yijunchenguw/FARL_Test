[{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"FARL: A Package for Large Scale Assessment","text":"tutorial, illustrate conduct multidimensional item response theory (MIRT) analysis multidimensional two parameter logistic (M2PL) multidimensional three parameter logistic (M3PL) models, differential item functioning (DIF) analysis M2PL models using VEMIRT package R, can installed package requires C++ compiler work properly, users referred https://github.com/MAP-LAB-UW/VEMIRT information. functions based Gaussian variational expectation-maximization (GVEM) algorithm, applicable high-dimensional latent traits.","code":"if (!require(devtools)) install.packages(\"devtools\") devtools::install_github(\"MAP-LAB-UW/VEMIRT\", build_vignettes = T) torch::install_torch() library(VEMIRT)"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"data-input","dir":"Articles","previous_headings":"","what":"Data Input","title":"FARL: A Package for Large Scale Assessment","text":"Data required analysis summarized : take dataset D2PL_data example. simulated dataset DIF 2PL analysis. Responses N J binary matrix, N J numbers respondents items respectively. Currently, DIF functions C2PL_iw2 allow responses missing data, coded NA. example, N=1500 respondents J=20 items. CFA DIF rely J D binary loading indicator matrix specifying latent dimensions item loads , D number latent dimensions. latent traits D=2 dimensions . DIF analysis additionally needs group membership vector length N, whose elements integers 1 G, G number groups. G=3 groups example.","code":"head(D2PL_data$data) #>      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] [,15] [,16] [,17] [,18] [,19] #> [1,]    0    1    0    1    1    0    1    0    0     0     1     0     0     1     0     1     1     1     1 #> [2,]    0    0    0    0    1    0    0    0    0     0     0     0     0     1     0     0     0     0     0 #> [3,]    0    0    0    0    0    0    1    0    0     0     0     0     0     0     0     0     1     0     1 #> [4,]    0    0    1    0    1    0    0    0    0     1     1     1     1     1     1     1     0     1     0 #> [5,]    0    0    0    0    0    0    0    0    0     1     0     0     0     0     0     1     0     0     0 #> [6,]    1    1    1    1    1    0    1    1    1     1     1     1     1     1     1     1     1     1     0 #>      [,20] #> [1,]     0 #> [2,]     0 #> [3,]     0 #> [4,]     0 #> [5,]     0 #> [6,]     1 D2PL_data$model #>       [,1] [,2] #>  [1,]    1    0 #>  [2,]    0    1 #>  [3,]    1    0 #>  [4,]    0    1 #>  [5,]    1    0 #>  [6,]    0    1 #>  [7,]    1    0 #>  [8,]    0    1 #>  [9,]    1    0 #> [10,]    0    1 #> [11,]    1    0 #> [12,]    0    1 #> [13,]    1    0 #> [14,]    0    1 #> [15,]    1    0 #> [16,]    0    1 #> [17,]    1    0 #> [18,]    0    1 #> [19,]    1    0 #> [20,]    0    1 table(D2PL_data$group) #>  #>   1   2   3  #> 500 500 500"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"data-output","dir":"Articles","previous_headings":"","what":"Data Output","title":"FARL: A Package for Large Scale Assessment","text":"functions output estimates item parameters related parameters. addition, C2PL_gvem, C2PL_bs C2PL_iw2 able provide standard errors item parameter estimates.","code":""},{"path":[]},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"parallel-analysis","dir":"Articles","previous_headings":"Exploratory Factor Analysis","what":"Parallel Analysis","title":"FARL: A Package for Large Scale Assessment","text":"Parallel analysis can conducted determine number factors. Users can specify number simulated datasets, takes n.iter = 10 default.","code":"pa_poly(D2PL_data$data, n.iter = 5) #> Parallel analysis suggests that the number of factors =  2"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"m2pl-model","dir":"Articles","previous_headings":"Exploratory Factor Analysis","what":"M2PL Model","title":"FARL: A Package for Large Scale Assessment","text":"VEMIRT provides following functions conduct EFA M2PL model: Currently functions estimate standard errors item parameters. following examples use two simulated datasets, E2PL_data_C1 E2PL_data_C2, N=1000 respondents, J=30 items D=3 dimensions, items load different dimensions. E2PL_gvem_rot needs item responses number factors (domain), applies promax rotation (rot = \"Promax\") default. Another choice rot = \"cfQ\", performs CF-Quartimax rotation. E2PL_gvem_lasso E2PL_gvem_adaptlasso need item responses, constraint setting (constrain), binary matrix specifying constraints sub-matrix factor loading structure (indic). constrain either \"C1\" \"C2\" ensure identifiability. \"C1\", D\\times D sub-matrix indic identity matrix, indicating D items loads solely one factor. Notice first 3 rows E2PL_data_C1$model form identity matrix. \"C2\", D\\times D sub-matrix indic lower triangular matrix whose diagonal elements one, indicating D items loads one factor potentially factors well; non-zero elements diagonal penalized. identification \"C2\", another argument non_pen provided, specifies anchor item loads factors. following example, first 2 rows row form lower triangular matrix, non_pen can take integer 3 30. E2PL_gvem_adaptlasso needs additional tuning parameter, takes gamma = 2 default. Users referred @cho2024 algorithmic details. GVEM known produce biased estimates discrimination parameters, E2PL_iw helps reduce bias importance sampling [@ma2024].","code":"E2PL_gvem_rot(E2PL_data_C1$data, domain = 3) #>          a1      a2       a3       b #> 1   0.10594  1.5952 -0.10845  1.7290 #> 2   1.91968  0.0721 -0.05631 -1.3236 #> 3   0.07972 -0.0394  1.56129 -1.2989 #> 4   0.19942  1.8727 -0.05273 -0.8808 #> 5   1.73784  0.0441 -0.11143 -0.8111 #> 6  -0.05117  0.0230  1.63743  0.1912 #> 7  -0.05174  1.3707 -0.00506 -0.9726 #> 8   1.52949  0.1766 -0.03231  0.4670 #> 9  -0.01529  0.0186  1.42425  1.1823 #> 10  0.04563  1.4053  0.07149  1.0613 #> 11  1.64560 -0.0849  0.06168  0.8636 #> 12  0.08978  0.0130  1.65967 -0.6542 #> 13  0.10412  1.8613  0.09076  0.0380 #> 14  1.42716 -0.1292  0.18454  1.5696 #> 15  0.03152  0.0564  1.78979 -1.3211 #> 16 -0.03327  1.7001  0.17390 -1.9581 #> 17  1.84224 -0.0486 -0.00188  0.5343 #> 18 -0.00405 -0.0918  1.60662  0.5123 #> 19 -0.19546  1.8230 -0.07211  0.1182 #> 20  1.34056 -0.0278  0.07707  0.3125 #> 21  0.14051 -0.0800  1.71112  0.1102 #> 22 -0.06479  1.9194 -0.02918 -0.6766 #> 23  1.55836  0.2900  0.05540  1.0387 #> 24 -0.20321  0.1247  1.59738  0.0535 #> 25  0.04718  1.4758  0.06536  0.3251 #> 26  1.75190 -0.0335  0.07172 -0.0327 #> 27  0.11757  0.0217  1.65739  0.5990 #> 28  0.00903  1.7688  0.05774 -1.5098 #> 29  1.65340 -0.0803  0.04002 -1.0095 #> 30 -0.20175  0.0796  1.52666 -0.1739 E2PL_data_C1$model #>       [,1] [,2] [,3] #>  [1,]    1    0    0 #>  [2,]    0    1    0 #>  [3,]    0    0    1 #>  [4,]    1    1    1 #>  [5,]    1    1    1 #>  [6,]    1    1    1 #>  [7,]    1    1    1 #>  [8,]    1    1    1 #>  [9,]    1    1    1 #> [10,]    1    1    1 #> [11,]    1    1    1 #> [12,]    1    1    1 #> [13,]    1    1    1 #> [14,]    1    1    1 #> [15,]    1    1    1 #> [16,]    1    1    1 #> [17,]    1    1    1 #> [18,]    1    1    1 #> [19,]    1    1    1 #> [20,]    1    1    1 #> [21,]    1    1    1 #> [22,]    1    1    1 #> [23,]    1    1    1 #> [24,]    1    1    1 #> [25,]    1    1    1 #> [26,]    1    1    1 #> [27,]    1    1    1 #> [28,]    1    1    1 #> [29,]    1    1    1 #> [30,]    1    1    1 E2PL_data_C2$model #>       [,1] [,2] [,3] #>  [1,]    1    0    0 #>  [2,]    1    1    0 #>  [3,]    1    1    1 #>  [4,]    1    1    1 #>  [5,]    1    1    1 #>  [6,]    1    1    1 #>  [7,]    1    1    1 #>  [8,]    1    1    1 #>  [9,]    1    1    1 #> [10,]    1    1    1 #> [11,]    1    1    1 #> [12,]    1    1    1 #> [13,]    1    1    1 #> [14,]    1    1    1 #> [15,]    1    1    1 #> [16,]    1    1    1 #> [17,]    1    1    1 #> [18,]    1    1    1 #> [19,]    1    1    1 #> [20,]    1    1    1 #> [21,]    1    1    1 #> [22,]    1    1    1 #> [23,]    1    1    1 #> [24,]    1    1    1 #> [25,]    1    1    1 #> [26,]    1    1    1 #> [27,]    1    1    1 #> [28,]    1    1    1 #> [29,]    1    1    1 #> [30,]    1    1    1 result <- with(E2PL_data_C1, E2PL_gvem_lasso(data, model, constrain = \"C1\")) result #>       a1     a2   a3       b #> 1  1.572  0.000 0.00  1.7198 #> 2  0.000  1.923 0.00 -1.3243 #> 3  0.000  0.000 1.58 -1.2981 #> 4  1.928  0.000 0.00 -0.8756 #> 5  0.000  1.696 0.00 -0.8114 #> 6  0.000  0.000 1.62  0.1907 #> 7  1.337  0.000 0.00 -0.9712 #> 8  0.000  1.589 0.00  0.4653 #> 9  0.000  0.000 1.42  1.1816 #> 10 1.473  0.000 0.00  1.0629 #> 11 0.000  1.639 0.00  0.8623 #> 12 0.000  0.000 1.71 -0.6524 #> 13 1.964  0.000 0.00  0.0408 #> 14 0.000  1.459 0.00  1.5628 #> 15 0.000  0.000 1.84 -1.3199 #> 16 1.776  0.000 0.00 -1.9514 #> 17 0.000  1.807 0.00  0.5304 #> 18 0.000  0.000 1.55  0.5113 #> 19 1.812 -0.277 0.00  0.1169 #> 20 0.000  1.367 0.00  0.3120 #> 21 0.000  0.000 1.73  0.1104 #> 22 1.861  0.000 0.00 -0.6754 #> 23 0.322  1.560 0.00  1.0364 #> 24 0.000  0.000 1.54  0.0525 #> 25 1.544  0.000 0.00  0.3269 #> 26 0.000  1.775 0.00 -0.0335 #> 27 0.000  0.000 1.74  0.6011 #> 28 1.803  0.000 0.00 -1.5078 #> 29 0.000  1.633 0.00 -1.0087 #> 30 0.000 -0.204 1.57 -0.1730 with(E2PL_data_C2, E2PL_gvem_adaptlasso(data, model, constrain = \"C2\", non_pen = 3)) #>         a1    a2   a3       b #> 1   1.6019 0.000 0.00  1.7409 #> 2   0.4230 2.419 0.00 -1.1530 #> 3   0.0000 2.653 1.00 -1.2417 #> 4   1.9203 0.000 0.00 -0.8737 #> 5  -1.2687 2.575 0.00 -0.8181 #> 6  -0.1981 0.993 1.29  0.1904 #> 7   1.3404 0.000 0.00 -0.9736 #> 8  -0.8955 2.214 0.00  0.4638 #> 9  -0.2045 0.912 1.12  1.1810 #> 10  1.5077 0.000 0.00  1.0795 #> 11 -1.2717 2.493 0.00  0.8623 #> 12 -0.3468 1.254 1.32 -0.6612 #> 13  2.0240 0.000 0.00  0.0465 #> 14 -1.1050 2.213 0.00  1.5660 #> 15 -0.2273 1.169 1.39 -1.3111 #> 16  1.8359 0.000 0.00 -1.9954 #> 17 -1.3386 2.723 0.00  0.5314 #> 18 -0.3741 1.040 1.27  0.5081 #> 19  1.6743 0.000 0.00  0.1161 #> 20 -0.9951 2.069 0.00  0.3137 #> 21 -0.4841 1.336 1.34  0.1086 #> 22  1.8836 0.000 0.00 -0.6795 #> 23 -0.7954 2.335 0.00  1.0436 #> 24  0.0000 0.736 1.25  0.0521 #> 25  1.5476 0.000 0.00  0.3304 #> 26 -1.2496 2.608 0.00 -0.0336 #> 27 -0.3240 1.235 1.30  0.5954 #> 28  1.8113 0.000 0.00 -1.5157 #> 29 -1.2838 2.500 0.00 -1.0119 #> 30 -0.0444 0.706 1.23 -0.1758 E2PL_iw(E2PL_data_C1$data, result) #>      a1     a2   a3       b #> 1  1.69  0.000 0.00  1.8353 #> 2  0.00  2.050 0.00 -1.4302 #> 3  0.00  0.000 1.71 -1.4091 #> 4  2.05  0.000 0.00 -0.9806 #> 5  0.00  1.822 0.00 -0.9043 #> 6  0.00  0.000 1.74  0.2367 #> 7  1.46  0.000 0.00 -1.0768 #> 8  0.00  1.712 0.00  0.5549 #> 9  0.00  0.000 1.54  1.2904 #> 10 1.59  0.000 0.00  1.1686 #> 11 0.00  1.763 0.00  0.9668 #> 12 0.00  0.000 1.84 -0.7436 #> 13 2.09  0.000 0.00 -0.0156 #> 14 0.00  1.580 0.00  1.6785 #> 15 0.00  0.000 1.96 -1.4296 #> 16 1.90  0.000 0.00 -2.0679 #> 17 0.00  1.933 0.00  0.6236 #> 18 0.00  0.000 1.68  0.5942 #> 19 1.94 -0.191 0.00  0.0856 #> 20 0.00  1.486 0.00  0.3882 #> 21 0.00  0.000 1.85  0.1406 #> 22 1.99  0.000 0.00 -0.7758 #> 23 0.42  1.683 0.00  1.1470 #> 24 0.00  0.000 1.66  0.0641 #> 25 1.67  0.000 0.00  0.3795 #> 26 0.00  1.900 0.00  0.0194 #> 27 0.00  0.000 1.86  0.6879 #> 28 1.93  0.000 0.00 -1.6212 #> 29 0.00  1.757 0.00 -1.1104 #> 30 0.00 -0.117 1.69 -0.2293"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"m3pl-model","dir":"Articles","previous_headings":"Exploratory Factor Analysis","what":"M3PL Model","title":"FARL: A Package for Large Scale Assessment","text":"VEMIRT provides following functions conduct EFA M3PL model: following examples use two simulated datasets, E3PL_data_C1 E3PL_data_C2, N=1000 respondents, J=30 items D=3 dimensions, items load different dimensions. usage functions similar M2PL models, additional arguments required: size subsample iteration (samp = 50 default), forget rate stochastic algorithm (forgetrate = 0.51 default), mean variance normal distribution prior item difficulty parameters (mu_b sigma2_b), \\alpha \\beta parameters beta distribution prior guessing parameters (Alpha Beta). Still, E3PL_sgvem_adaptlasso needs tuning parameter, takes gamma = 2 default. Users referred @cho2024 algorithmic details. following examples, priors difficulty parameters guessing parameters N(0,2^2) \\beta(10,40) respectively.","code":"with(E3PL_data_C1, E3PL_sgvem_adaptlasso(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40, constrain = \"C1\")) #>       a1    a2    a3       b     c #> 1  0.794 0.000 0.000  0.1891 0.185 #> 2  0.000 1.290 0.000 -1.2229 0.188 #> 3  0.000 0.000 1.374 -1.2121 0.187 #> 4  1.755 0.000 0.000 -0.6975 0.181 #> 5  0.000 1.382 0.000 -1.0080 0.187 #> 6  0.000 0.000 1.311  0.0494 0.182 #> 7  1.480 0.000 0.000 -0.9579 0.184 #> 8  0.000 1.148 0.000  0.2136 0.182 #> 9  0.000 0.000 0.903  0.7030 0.180 #> 10 1.152 0.000 0.000  0.6787 0.177 #> 11 0.000 1.281 0.000  0.7833 0.172 #> 12 0.000 0.000 1.170 -0.8145 0.187 #> 13 1.865 0.000 0.000  0.4726 0.162 #> 14 0.000 0.965 0.000  1.0470 0.174 #> 15 0.000 0.000 1.239 -1.0900 0.188 #> 16 1.437 0.000 0.000 -1.5191 0.188 #> 17 0.000 1.480 0.000  0.1825 0.176 #> 18 0.000 0.000 1.080  0.4050 0.180 #> 19 1.445 0.000 0.000 -0.1578 0.179 #> 20 0.000 1.110 0.000  0.2268 0.182 #> 21 0.000 0.000 1.343  0.1158 0.176 #> 22 1.728 0.000 0.000 -0.3874 0.176 #> 23 0.000 1.195 0.000  0.9668 0.175 #> 24 0.000 0.000 1.053 -0.5243 0.186 #> 25 1.393 0.000 0.000  0.2389 0.176 #> 26 0.000 1.294 0.000  0.2698 0.180 #> 27 0.000 0.000 1.095  0.0491 0.182 #> 28 1.403 0.000 0.000 -1.2013 0.187 #> 29 0.000 1.182 0.000 -0.8919 0.186 #> 30 0.000 0.000 1.070 -0.2430 0.184 with(E3PL_data_C2, E3PL_sgvem_lasso(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40, constrain = \"C2\", non_pen = 3)) #>        a1      a2     a3       b     c #> 1   0.651  0.0000 0.0000  0.1974 0.183 #> 2   0.000  1.4557 0.0000 -1.2644 0.183 #> 3   0.000  0.0000 1.6780 -1.0679 0.181 #> 4   1.134  0.5253 0.0000 -0.7284 0.182 #> 5  -0.213  1.2971 0.0000 -0.9946 0.185 #> 6   0.000 -0.3714 1.5183  0.0309 0.179 #> 7   0.911  0.3709 0.1708 -1.0041 0.184 #> 8  -0.396  1.2365 0.0000  0.1655 0.177 #> 9   0.000  0.0000 0.8501  0.7159 0.177 #> 10  0.817  0.2930 0.0000  0.6894 0.175 #> 11 -0.275  1.3122 0.0000  0.7671 0.168 #> 12  0.000  0.0000 1.1435 -0.8549 0.186 #> 13  1.114  0.4229 0.3675  0.5424 0.162 #> 14  0.110  0.7856 0.0000  1.0758 0.172 #> 15  0.000 -0.3320 1.4868 -1.1485 0.186 #> 16  0.843  0.4697 0.1483 -1.5600 0.187 #> 17 -0.296  1.3820 0.0000  0.1664 0.173 #> 18  0.000 -0.0138 0.9824  0.4223 0.180 #> 19  0.933  0.4708 0.0000 -0.1704 0.179 #> 20 -0.327  1.1643 0.0000  0.1555 0.179 #> 21  0.000 -0.3426 1.5295  0.1104 0.173 #> 22  1.109  0.5069 0.0497 -0.4049 0.177 #> 23 -0.200  1.1294 0.0000  0.9396 0.172 #> 24  0.000  0.0000 0.9778 -0.5629 0.187 #> 25  1.007  0.3174 0.0000  0.2351 0.175 #> 26 -0.347  1.3133 0.0000  0.1969 0.177 #> 27  0.000 -0.1086 1.1230  0.0685 0.182 #> 28  1.029  0.2085 0.0863 -1.2286 0.186 #> 29 -0.138  1.0991 0.0000 -0.9710 0.186 #> 30  0.000 -0.3731 1.2805 -0.2715 0.182"},{"path":[]},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"m2pl-model-1","dir":"Articles","previous_headings":"Confirmatory Factor Analysis","what":"M2PL Model","title":"FARL: A Package for Large Scale Assessment","text":"VEMIRT provides following functions conduct CFA M2PL model: binary loading indicator matrix needs provided CFA. C2PL_gvem can produce biased estimates two functions help reduce bias. Also, C2PL_gvem, C2PL_bs C2PL_iw2 able provide standard errors item parameters. C2PL_iw C2PL_iw2 apply almost algorithm different implementations. specifically, C2PL_iw2 calls D2PL_gvem estimation, unlike C2PL_iw uses stochastic gradient descent resampling posteriors latent traits iteration, C2PL_iw2 samples posteriors tends less stable accurate. Users referred @cho2021 C2PL_gvem @ma2024 C2PL_iw C2PL_iw2. following examples use simulated dataset, C2PL_data, N=1000 respondents, J=20 items D=2 dimensions.","code":"result <- with(C2PL_data, C2PL_gvem(data, model)) result #>      a1   a2       b #> 1  1.91 0.00  0.8789 #> 2  0.00 1.71 -2.1325 #> 3  1.85 0.00  0.4760 #> 4  0.00 2.03 -1.2061 #> 5  1.89 0.00 -0.6088 #> 6  0.00 1.57  1.1850 #> 7  1.46 0.00 -1.8576 #> 8  0.00 1.79 -0.3041 #> 9  1.40 0.00  0.3218 #> 10 0.00 1.49 -0.4536 #> 11 1.89 0.00 -0.3825 #> 12 0.00 1.71 -0.1322 #> 13 1.36 0.00  0.5896 #> 14 0.00 1.43 -2.1947 #> 15 1.80 0.00  0.0357 #> 16 0.00 1.51  0.9673 #> 17 1.97 0.00  0.1262 #> 18 0.00 1.68  0.5347 #> 19 1.81 0.00 -0.7190 #> 20 0.00 1.62 -1.4178 C2PL_bs(result) #>      a1   a2       b #> 1  2.22 0.00  1.0157 #> 2  0.00 2.04 -2.3936 #> 3  2.22 0.00  0.5223 #> 4  0.00 2.34 -1.3797 #> 5  2.23 0.00 -0.6784 #> 6  0.00 1.74  1.2075 #> 7  1.67 0.00 -1.9693 #> 8  0.00 2.05 -0.3203 #> 9  1.55 0.00  0.3333 #> 10 0.00 1.74 -0.5015 #> 11 2.24 0.00 -0.4104 #> 12 0.00 1.99 -0.1192 #> 13 1.46 0.00  0.6153 #> 14 0.00 1.69 -2.3542 #> 15 2.11 0.00  0.0356 #> 16 0.00 1.71  0.9975 #> 17 2.24 0.00  0.1562 #> 18 0.00 1.81  0.4763 #> 19 2.06 0.00 -0.7782 #> 20 0.00 1.84 -1.5228 C2PL_iw(C2PL_data$data, result) #>      a1   a2       b #> 1  2.16 0.00  1.0352 #> 2  0.00 1.96 -2.3604 #> 3  2.09 0.00  0.5737 #> 4  0.00 2.28 -1.4103 #> 5  2.14 0.00 -0.7289 #> 6  0.00 1.80  1.3748 #> 7  1.70 0.00 -2.0733 #> 8  0.00 2.03 -0.4369 #> 9  1.63 0.00  0.3934 #> 10 0.00 1.72 -0.5925 #> 11 2.14 0.00 -0.4616 #> 12 0.00 1.95 -0.2265 #> 13 1.58 0.00  0.7169 #> 14 0.00 1.67 -2.4219 #> 15 2.05 0.00  0.0132 #> 16 0.00 1.74  1.1379 #> 17 2.22 0.00  0.1296 #> 18 0.00 1.92  0.6167 #> 19 2.06 0.00 -0.8566 #> 20 0.00 1.86 -1.6292 with(C2PL_data, C2PL_iw2(data, model, SE.level = 10)) #>            1      2     3      4      5     6      7      8      9      10     11      12     13     14     15 #> a1     2.328  0.000 2.233  0.000  2.307 0.000  1.662  0.000 1.5332  0.0000  2.307  0.0000 1.4889  0.000 2.1671 #> a2     0.000  2.114 0.000  2.586  0.000 1.748  0.000  2.133 0.0000  1.6787  0.000  2.0354 0.0000  1.700 0.0000 #> b      0.954 -2.390 0.507 -1.415 -0.686 1.231 -1.966 -0.359 0.3214 -0.4958 -0.437 -0.1682 0.5991 -2.378 0.0256 #> SE(a1) 0.189  0.000 0.185  0.000  0.186 0.000  0.157  0.000 0.1251  0.0000  0.189  0.0000 0.1256  0.000 0.1678 #> SE(a2) 0.000  0.191 0.000  0.228  0.000 0.155  0.000  0.179 0.0000  0.1341  0.000  0.1601 0.0000  0.157 0.0000 #> SE(b)  0.119  0.172 0.108  0.141  0.110 0.109  0.138  0.105 0.0864  0.0904  0.109  0.0977 0.0886  0.150 0.0999 #>           16    17     18     19    20 #> a1     0.000 2.431 0.0000  2.172  0.00 #> a2     1.655 0.000 1.9226  0.000  1.93 #> b      0.991 0.123 0.5467 -0.796 -1.56 #> SE(a1) 0.000 0.194 0.0000  0.177  0.00 #> SE(a2) 0.149 0.000 0.1587  0.000  0.17 #> SE(b)  0.104 0.108 0.0975  0.109  0.13"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"m3pl-model-1","dir":"Articles","previous_headings":"Confirmatory Factor Analysis","what":"M3PL Model","title":"FARL: A Package for Large Scale Assessment","text":"C3PL_sgvem conducts CFA M3PL models. usage similar E3PL_sgvem_* except binary loading indicator matrix needed additionally. Users referred @cho2021 algorithmic details. following example uses simulated dataset, C3PL_data, N=2000 respondents, J=20 items D=2 dimensions. priors difficulty parameters guessing parameters chosen N(0,2^2) \\beta(10,40) respectively.","code":"with(C3PL_data, C3PL_sgvem(data, model, mu_b = 0, sigma2_b = 4, Alpha = 10, Beta = 40)) #>      a1    a2       b     c #> 1  1.16 0.000 -0.1390 0.185 #> 2  0.00 1.451  1.5487 0.151 #> 3  1.73 0.000 -0.5207 0.179 #> 4  0.00 1.494 -1.5277 0.187 #> 5  1.38 0.000 -1.6949 0.188 #> 6  0.00 1.336  1.5977 0.156 #> 7  1.26 0.000 -0.6656 0.186 #> 8  0.00 1.385  0.0985 0.178 #> 9  1.66 0.000  0.6166 0.172 #> 10 0.00 1.390 -0.1053 0.182 #> 11 1.19 0.000  0.4890 0.177 #> 12 0.00 0.924  1.3498 0.173 #> 13 1.42 0.000  1.4915 0.157 #> 14 0.00 1.165  0.8381 0.174 #> 15 1.20 0.000 -0.0716 0.181 #> 16 0.00 1.498  0.6667 0.168 #> 17 1.40 0.000 -0.4564 0.182 #> 18 0.00 1.676 -1.2551 0.183 #> 19 1.01 0.000  1.0441 0.173 #> 20 0.00 1.133 -0.7544 0.186"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"differential-item-functioning","dir":"Articles","previous_headings":"","what":"Differential Item Functioning","title":"FARL: A Package for Large Scale Assessment","text":"VEMIRT provides following functions detect DIF M2PL model: Currently D2PL_pair_em supports unidimensional latent trait , strongly recommended D=1 produces accurate estimates allows comparison every pair groups. D2PL_pair_em requires item responses group membership input. require loading indicator assumes every item loads single dimension. following example, responses generated two-dimensional latent traits correlation 0.8, treat one dimension. Note truncated lasso (L_1) penalty becomes lasso penalty tau takes Inf. following example, DIF detection results ordered pairs groups DIF parameters flagged X. D2PL_pair_em fit need, recommend D2PL_em low-dimensional cases (e.g., D\\leq 3) accurate; D2PL_gvem recommend high-dimensional cases /fast estimation. functions require item responses, loading indicator, group membership. Besides, estimation method (method) tuning parameter vector (Lambda0) two important arguments. EMM IWGVEMM default choices recommended D2PL_em D2PL_gvem respectively methods accurate. Specifically, IWGVEMM additional importance sampling step GVEM estimation. recommend D2PL_lrt time-consuming. Users referred @wang2023 D2PL_em @lyu2025 D2PL_gvem. example , results ordered groups group 1 reference group. DIF parameters flagged X, indicating item parameter group different group 1. default, D2PL_em D2PL_pair_em choose best tuning parameters using Bayesian information criterion (BIC), D2PL_gvem uses generalized information criterion (GIC) c=1. example 0.5 chosen, AIC, BIC, GIC values c can also used specifying \"AIC\", \"BIC\", value c functions coef, print summary. suggest c 0 1, larger values lead lower true false positive rates. message warns us optimal tuning parameter AIC may range specified estimation. Users specify wider range corresponding argument current information criterion used. Finally, parameter estimates can obtained :","code":"with(D2PL_data, D2PL_pair_em(data, group, Lambda0 = seq(1, 1.5, by = 0.1), Tau = c(Inf, seq(0.002, 0.01, by = 0.002)), verbose = FALSE)) #>        1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #> 1,2:a1                                                    #> 1,2:b  X X   X X                                          #> 1,3:a1       X                                            #> 1,3:b  X X X X X X                                        #> 2,3:a1       X                                            #> 2,3:b  X   X X X X                                        #> * lambda0 = 1.2, tau = 0.004 result <- with(D2PL_data, D2PL_gvem(data, model, group, method = 'IWGVEMM', Lambda0 = seq(0.2, 0.7, by = 0.1), verbose = F)) result #>      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #> 2:a1                                                    #> 2:a2                                                    #> 2:b          X                                          #> 3:a1                                                    #> 3:a2       X                                            #> 3:b  X X X X X X                                        #> * lambda0 = 0.5 summary(result, 0.5) #>    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #> a1         X                                          #> a2       X   X                                        #> b  X X X X X X   X    X                               #> * lambda0 = 0.3 print(result, 'AIC') #> Warning in check.vemirt_DIF(all, fit, \"lambda0\"): Optimal lambda0 may be less than 0.2. #>      1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #> 2:a1                                                    #> 2:a2                                                    #> 2:b  X X   X X   X      X        X     X  X     X       #> 3:a1     X   X       X                                  #> 3:a2   X   X   X                                        #> 3:b  X X X X X X   X X  X                       X     X #> * lambda0 = 0.2 str(coef(result, 'BIC')) #> List of 17 #>  $ lambda0: num 0.3 #>  $ lambda : num 11.6 #>  $ niter  : num [1:2] 109 104 #>  $ ll     : num -11570 #>  $ l0     : int 14 #>  $ SIGMA  : num [1:1500, 1:2, 1:2] 0.103 0.124 0.116 0.103 0.13 ... #>  $ MU     : num [1:1500, 1:2] 0.00366 -1.21956 -0.86532 0.05182 -1.44339 ... #>  $ Sigma  : num [1:3, 1:2, 1:2] 1 0.991 1.034 0.829 0.823 ... #>  $ Mu     : num [1:3, 1:2] 0 -0.0405 0.0701 0 -0.1188 ... #>  $ a      : num [1:20, 1:2] 2.28 0 2.09 0 1.95 ... #>  $ b      : num [1:20] 1.0022 0.649 0.0387 0.8109 -0.1485 ... #>  $ gamma  : num [1:3, 1:20, 1:2] 0 0 0 0 0 0 0 0 0 0 ... #>  $ beta   : num [1:3, 1:20] 0 0 0.853 0 0.608 ... #>  $ RMSE   : num 0.0173 #>  $ AIC    : num 23168 #>  $ BIC    : num 23242 #>  $ GIC    : num 23343"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"package-evaluation","dir":"Articles","previous_headings":"","what":"Package Evaluation","title":"FARL: A Package for Large Scale Assessment","text":"show two examples test VEMIRT package simulating data, estimating model using VEMIRT, checking accuracy.","code":"library(abind) library(mvtnorm)"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"confirmatory-2pl-model","dir":"Articles","previous_headings":"Package Evaluation","what":"Confirmatory 2PL Model","title":"FARL: A Package for Large Scale Assessment","text":"","code":"set.seed(1) Sigma <- matrix(c(1, 0.85, 0.85, 1), 2) J <- 10 N <- 1000 model <- cbind(rep(1:0, J / 2), rep(0:1, J / 2)) a <- matrix(runif(J * 2, 1, 3), ncol = 2) * model b <- rnorm(J) theta <- rmvnorm(N, rep(0, 2), Sigma) data <- t(matrix(rbinom(N * J, 1, plogis(a %*% t(theta) - b)), nrow = J))  result.gvem <- C2PL_gvem(data, model) result.iw <- C2PL_iw(data, result.gvem) result.iw2 <- C2PL_iw2(data, model)  rmse <- function(x, y) {   sqrt(mean((x - y) ^ 2)) } c(a = rmse(a[model == 1], coef(result.gvem)[, 1:2][model == 1]), b = rmse(b, coef(result.gvem)$b)) #>         a         b  #> 0.6039514 0.1318309 c(a = rmse(a[model == 1], coef(result.iw)[, 1:2][model == 1]), b = rmse(b, coef(result.iw)$b)) #>          a          b  #> 0.40411993 0.09705373 c(a = rmse(a[model == 1], coef(result.iw2)$a[model == 1]), b = rmse(b, coef(result.iw2)$b)) #>         a         b  #> 0.1370900 0.0771889"},{"path":"http://127.0.0.1:8000/articles/FARL.html","id":"dif-2pl-model","dir":"Articles","previous_headings":"Package Evaluation","what":"DIF 2PL Model","title":"FARL: A Package for Large Scale Assessment","text":"","code":"set.seed(1) Sigma <- matrix(c(1, 0.85, 0.85, 1), 2) J <- 10 j <- J * 0.4 n <- 300 group <- rep(1:3, each = n) model <- cbind(rep(1:0, J / 2), rep(0:1, J / 2)) a <- matrix(runif(J * 2, 1, 3), ncol = 2) * model a <- unname(abind(a, a, a, along = 0)) a[-1, 1:(j / 2), ] <- a[-1, 1:(j / 2), ] + c(0.5, 1) a[-1, (j / 2 + 1):j, ] <- a[-1, (j / 2 + 1):j, ] - c(0.5, 1) a[-1, , ] <- a[-1, , ] * abind(model, model, along = 0) b <- rnorm(J) b <- unname(rbind(b, b, b)) b[-1, 1:(j / 2)] <- b[-1, 1:(j / 2)] - c(0.5, 1) b[-1, (j / 2 + 1):j] <- b[-1, (j / 2 + 1):j] + c(0.5, 1) theta <- rmvnorm(n * 3, rep(0, 2), Sigma) data <- t(sapply(1:(n * 3), function(n) {   rbinom(J, 1, plogis(a[group[n], , ] %*% theta[n, ] - b[group[n], ])) }))  result.iw <- D2PL_gvem(data, model, group, verbose = F) result.iw.gic_0.3 <- summary(result.iw, 0.3) result.iw.gic_1 <- summary(result.iw, 1) result.iw.bic <- summary(result.iw, 'BIC')  count <- function(j, result) {   pos <- colSums(result) > 0   c(`True Positive` = mean(pos[1:j]), `False Positive` = mean(pos[-(1:j)])) } count(j, coef(result.iw.gic_0.3)) #>  True Positive False Positive  #>            1.0            0.5 count(j, coef(result.iw.gic_1)) #>  True Positive False Positive  #>           0.75           0.00 count(j, coef(result.iw.bic)) #>  True Positive False Positive  #>      1.0000000      0.1666667"},{"path":[]},{"path":"http://127.0.0.1:8000/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Yijun Cheng. Author, maintainer.","code":""},{"path":"http://127.0.0.1:8000/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Cheng Y (2026). FARL: Factor-Adjusted Regularized Latent Regression. R package version 0.1.0.","code":"@Manual{,   title = {FARL: Factor-Adjusted Regularized Latent Regression},   author = {Yijun Cheng},   year = {2026},   note = {R package version 0.1.0}, }"},{"path":"http://127.0.0.1:8000/index.html","id":"farl-use-factor-augmented-regularized-latent-regression-for-large-scale-assessment-","dir":"","previous_headings":"","what":"Factor-Adjusted Regularized Latent Regression","title":"Factor-Adjusted Regularized Latent Regression","text":"goal FARL provide computationally efficient tools large scale assessment, generate Plausible Values, high accuracy. package contains several example datasets functions FARLR: Factor-augmented Regularized Latent Regression FARLR-debias: Factor-augmented Regularized Latent Regression - Debias","code":""},{"path":"http://127.0.0.1:8000/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Factor-Adjusted Regularized Latent Regression","text":"install package source: Windows users may need install Rtools include checkbox option installing Rtools path easier command line usage. Mac users may need install Xcode command line tools sudo xcode-select --install terminal, install GNU Fortran compiler. Linux distributions already date compilers (can installed/updated easily). Install devtools package (necessary), install package GitHub ","code":"if (!require(devtools)) install.packages(\"devtools\") devtools::install_github(\"MAP-LAB-UW/FARL\", build_vignettes = T) torch::install_torch()"},{"path":"http://127.0.0.1:8000/index.html","id":"tutorial","dir":"","previous_headings":"","what":"Tutorial","title":"Factor-Adjusted Regularized Latent Regression","text":"installing FARL package, open tutorial running","code":"vignette(\"FARL\")"},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":null,"dir":"Reference","previous_headings":"","what":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"FARL developed support large-scale assessment (LSA) analyses, latent regression models used integrate students’ background information generate plausible values (PVs) secondary analysis. LSA settings, background variables often high-dimensional highly correlated, posing substantial challenges traditional latent regression approaches.","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"FARL implements factor-augmented regularized latent regression (FARLR), innovative framework jointly models common factors idiosyncratic components. Regularization employed select relevant idiosyncratic predictors, yielding interpretable regression results maintaining congeniality estimation stability.","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"latent-regression-models","dir":"Reference","previous_headings":"","what":"Latent regression models","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"FARLR_mml fits factor-augmented regularized latent regression model via marginal maximum likelihood","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"debiasing-and-bias-correction","dir":"Reference","previous_headings":"","what":"Debiasing and bias correction","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"FARLR_mml_debias applies debiasing corrections FARLR estimates","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"plausible-value-generation","dir":"Reference","previous_headings":"","what":"Plausible value generation","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"drawPVs generates plausible values FARLR framework","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"simulation-and-example-data","dir":"Reference","previous_headings":"","what":"Simulation and example data","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"Built-example datasets illustrating FARLR estimation PV generation Utility functions simulation studies large-scale assessment settings","code":""},{"path":"http://127.0.0.1:8000/reference/FARL-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"FARL: Factor-Augmented Regularized Latent Regression for Large-Scale Assessment — FARL-package","text":"Maintainer: Yijun Cheng chengxb@uw.edu (ORCID)","code":""},{"path":"http://127.0.0.1:8000/reference/drawPVs.html","id":null,"dir":"Reference","previous_headings":"","what":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","title":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","text":"Generates plausible values (PVs) latent abilities based fitted marginal maximum likelihood (MML) EM-based latent variable model. PVs drawn subject-specific normal distributions using posterior mean variance estimates.","code":""},{"path":"http://127.0.0.1:8000/reference/drawPVs.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","text":"","code":"drawPVs(mmlcomp, npv = 10L, verbose = TRUE)"},{"path":"http://127.0.0.1:8000/reference/drawPVs.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","text":"mmlcomp object returned latent variable model fitting function (e.g., fit_gvem related EM-based estimator), contains posterior summaries latent abilities. npv Integer. Number plausible values draw subject. Default 10. verbose Logical. TRUE, prints progress information plausible value generation. Default TRUE.","code":""},{"path":"http://127.0.0.1:8000/reference/drawPVs.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","text":"data frame containing plausible values, one row per subject   npv columns corresponding generated plausible values.","code":""},{"path":"http://127.0.0.1:8000/reference/drawPVs.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Draw Plausible Values from a Fitted Latent Variable Model — drawPVs","text":"Plausible values generated sampling posterior predictive distribution latent abilities. subject, draws taken independently normal distribution parameterized posterior mean variance obtained fitted model. approach propagates measurement uncertainty secondary analyses.","code":""},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":null,"dir":"Reference","previous_headings":"","what":"Factor analysis — factor.analysis","title":"Factor analysis — factor.analysis","text":"main function factor analysis potentially high dimensional variables. implement recent algorithms optimized high dimensional problem number samples n less number variables p.","code":""},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Factor analysis — factor.analysis","text":"","code":"factor.analysis(Y, r, method = c(\"ml\", \"pc\", \"esa\"))"},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Factor analysis — factor.analysis","text":"Y data matrix, n*p matrix r number factors method algorithm used","code":""},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Factor analysis — factor.analysis","text":"list objects Gamma estimated factor loadings Z estimated latent factors Sigma estimated noise variance matrix","code":""},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Factor analysis — factor.analysis","text":"three methods quasi-maximum likelihood (ml), principal component analysis (pc), factor analysis using early stopping criterion (esa). ml iteratively solved Expectation-Maximization algorithm using PCA solution initial value. See Bai Li (2012) details. esa method, see Owen Wang (2015) details.","code":""},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Factor analysis — factor.analysis","text":"Bai, J. Li, K. (2012). Statistical analysis factor models high dimension. Annals Statistics 40, 436-465. Owen, . B. Wang, J. (2015). Bi-cross-validation factor analysis. arXiv:1503.03515.","code":""},{"path":[]},{"path":"http://127.0.0.1:8000/reference/factor.analysis.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Factor analysis — factor.analysis","text":"","code":"## a factor model n <- 100 p <- 1000 r <- 5 Z <- matrix(rnorm(n * r), n, r) Gamma <- matrix(rnorm(p * r), p, r) Y <- Z %*% t(Gamma) + rnorm(n * p)  ## to check the results, verify the true factors are in the linear span of the estimated factors. pc.results <- factor.analysis(Y, r = 5, \"pc\") sapply(summary(lm(Z ~ pc.results$Z)), function(x) x$r.squared) #> Response Y1 Response Y2 Response Y3 Response Y4 Response Y5  #>   0.9990319   0.9991650   0.9990495   0.9990166   0.9990453   ml.results <- factor.analysis(Y, r = 5, \"ml\") sapply(summary(lm(Z ~ ml.results$Z)), function(x) x$r.squared) #> Response Y1 Response Y2 Response Y3 Response Y4 Response Y5  #>   0.9990405   0.9991530   0.9989957   0.9989903   0.9989799   esa.results <- factor.analysis(Y, r = 5, \"esa\") #> Error in ESA(Y, r): could not find function \"ESA\" sapply(summary(lm(Z ~ esa.results$Z)), function(x) x$r.squared) #> Error in eval(predvars, data, env): object 'esa.results' not found"},{"path":"http://127.0.0.1:8000/reference/farlr_em.html","id":null,"dir":"Reference","previous_headings":"","what":"FARLR Approach II via EM — farlr_em","title":"FARLR Approach II via EM — farlr_em","text":"Implements FARLR Approach II using EM algorithm. method estimates latent regression parameters covariates structured covariance supports iterative updating convergence control.","code":""},{"path":"http://127.0.0.1:8000/reference/farlr_em.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"FARLR Approach II via EM — farlr_em","text":"","code":"farlr_em(   n,   resp,   parTab,   K_hat,   p,   lambda_all,   delta.criteria = 0.001,   iter.max = 500,   n_sam = 50,   window.size = 50,   theta_est_irt.mean,   theta_est_irt.se,   resp_rep,   Z.em,   main,   verbose = TRUE )"},{"path":"http://127.0.0.1:8000/reference/farlr_em.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"FARLR Approach II via EM — farlr_em","text":"n Integer. Sample size. resp Matrix. Item response matrix (individuals \\(\\times\\) items). K_hat Integer. Estimated number latent dimensions. p Integer. Number covariates. lambda_all Numeric vector matrix. Regularization parameters. delta.criteria Numeric. Convergence threshold parameter updates. Default 1e-3. iter.max Integer. Maximum number EM iterations. Default 500. n_sam Integer. Number Monte Carlo posterior samples. Default 50. window.size Integer. Window size used convergence diagnostics. Default 50. theta_est_irt.mean Numeric matrix. Initial latent ability means obtained IRT model. theta_est_irt.se Numeric matrix. Standard errors IRT-based latent ability estimates. resp_rep Matrix. Replicated augmented response matrix used EM. Z.em Matrix. Design matrix EM-based latent regression. main Logical. TRUE, uses main effects regression. Numeric vector matrix. Item discrimination parameters. d Numeric vector matrix. Item difficulty (intercept) parameters. c Numeric vector. Guessing parameters (applicable).","code":""},{"path":"http://127.0.0.1:8000/reference/farlr_em.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"FARLR Approach II via EM — farlr_em","text":"list containing: coef: Estimated coefficients latent regression model. sigma: Estimated covariance matrix latent variables. minBIC: Minimum Bayesian Information Criterion (BIC) value achieved model fitting. converged: Logical indicator whether EM algorithm converged. cov: Number EM iterations required convergence.","code":""},{"path":"http://127.0.0.1:8000/reference/farlr_em.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"FARLR Approach II via EM — farlr_em","text":"FARLR Approach II models covariate-dependent heteroskedasticity low-rank random-effects formulation latent covariance structure. Estimation carried via Gaussian Variational EM algorithm, closed-form updates available latent regression covariance parameters, convergence monitored using sliding window criterion.","code":""},{"path":"http://127.0.0.1:8000/reference/mml.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a Latent Regression Model via the EM Algorithm — mml","title":"Fit a Latent Regression Model via the EM Algorithm — mml","text":"Fits latent regression model using expectation–maximization (EM) algorithm, latent abilities linked observed covariates item response data IRT measurement model.","code":""},{"path":"http://127.0.0.1:8000/reference/mml.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a Latent Regression Model via the EM Algorithm — mml","text":"","code":"mml(X, Y, parTab, p, n_sam = 30, verbose = TRUE)"},{"path":"http://127.0.0.1:8000/reference/mml.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a Latent Regression Model via the EM Algorithm — mml","text":"X Numeric matrix. Design matrix covariates (\\(N \\times p\\)). Y Matrix. Item response matrix (\\(N \\times J\\)). p Integer. Number covariates included latent regression. n_sam Integer. Number Monte Carlo quadrature samples used approximate posterior expectations E-step. Default 30. verbose Logical. TRUE, prints progress information estimation. Default TRUE. Numeric vector matrix. Item discrimination parameters. d Numeric vector matrix. Item difficulty (intercept) parameters.","code":""},{"path":"http://127.0.0.1:8000/reference/mml.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a Latent Regression Model via the EM Algorithm — mml","text":"list containing: coef: Estimated coefficients latent regression model. Sigma: Estimated covariance matrix latent variables. theta: Posterior mean estimates latent abilities. converged: Logical indicator whether EM algorithm     converged. iter: Number EM iterations performed.","code":""},{"path":"http://127.0.0.1:8000/reference/mml.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a Latent Regression Model via the EM Algorithm — mml","text":"EM algorithm alternates computing conditional expectations latent abilities given observed responses current parameter estimates (E-step) maximizing expected complete-data log-likelihood respect model parameters (M-step). Numerical optimization employed item parameters, closed-form updates available latent regression covariance parameters.","code":""}]
